{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_ranking'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c7b58a9bb643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_ranking\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_ranking'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import pandas as pd\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-15 12:50:25--  https://raw.githubusercontent.com/tensorflow/ranking/master/tensorflow_ranking/examples/data/train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29788 (29K) [text/plain]\n",
      "Saving to: ‘/tmp/train.txt’\n",
      "\n",
      "/tmp/train.txt      100%[===================>]  29.09K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2019-05-15 12:50:26 (3.19 MB/s) - ‘/tmp/train.txt’ saved [29788/29788]\n",
      "\n",
      "--2019-05-15 12:50:26--  https://raw.githubusercontent.com/tensorflow/ranking/master/tensorflow_ranking/examples/data/test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8959 (8.7K) [text/plain]\n",
      "Saving to: ‘/tmp/test.txt’\n",
      "\n",
      "/tmp/test.txt       100%[===================>]   8.75K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-05-15 12:50:26 (66.0 MB/s) - ‘/tmp/test.txt’ saved [8959/8959]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O \"/tmp/train.txt\" \"https://raw.githubusercontent.com/tensorflow/ranking/master/tensorflow_ranking/examples/data/train.txt\"\n",
    "! wget -O \"/tmp/test.txt\" \"https://raw.githubusercontent.com/tensorflow/ranking/master/tensorflow_ranking/examples/data/test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  EXAMPLE\n",
    "\n",
    "# Store the paths to files containing training and test instances.\n",
    "# As noted above, we will assume the data is in the LibSVM format\n",
    "# and that the content of each file is sorted by query ID.\n",
    "_TRAIN_DATA_PATH=\"/tmp/train.txt\"\n",
    "_TEST_DATA_PATH=\"/tmp/test.txt\"\n",
    "\n",
    "# Define a loss function. To find a complete list of available\n",
    "# loss functions or to learn how to add your own custom function\n",
    "# please refer to the tensorflow_ranking.losses module.\n",
    "_LOSS=\"pairwise_logistic_loss\"\n",
    "\n",
    "# In the TF-Ranking framework, a training instance is represented\n",
    "# by a Tensor that contains features from a list of documents\n",
    "# associated with a single query. For simplicity, we fix the shape\n",
    "# of these Tensors to a maximum list size and call it \"list_size,\"\n",
    "# the maximum number of documents per query in the dataset.\n",
    "# In this demo, we take the following approach:\n",
    "#   * If a query has fewer documents, its Tensor will be padded\n",
    "#     appropriately.\n",
    "#   * If a query has more documents, we shuffle its list of\n",
    "#     documents and trim the list down to the prescribed list_size.\n",
    "_LIST_SIZE=100\n",
    "\n",
    "# The total number of features per query-document pair.\n",
    "# We set this number to the number of features in the MSLR-Web30K\n",
    "# dataset.\n",
    "_NUM_FEATURES=136\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE=32\n",
    "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  EXAMPLE 10 lines\n",
    "\n",
    "# Store the paths to files containing training and test instances.\n",
    "# As noted above, we will assume the data is in the LibSVM format\n",
    "# and that the content of each file is sorted by query ID.\n",
    "_TRAIN_DATA_PATH=\"/tmp/train_10.txt\"\n",
    "_TEST_DATA_PATH=\"/tmp/test_10.txt\"\n",
    "\n",
    "# Define a loss function. To find a complete list of available\n",
    "# loss functions or to learn how to add your own custom function\n",
    "# please refer to the tensorflow_ranking.losses module.\n",
    "_LOSS=\"pairwise_logistic_loss\"\n",
    "\n",
    "# In the TF-Ranking framework, a training instance is represented\n",
    "# by a Tensor that contains features from a list of documents\n",
    "# associated with a single query. For simplicity, we fix the shape\n",
    "# of these Tensors to a maximum list size and call it \"list_size,\"\n",
    "# the maximum number of documents per query in the dataset.\n",
    "# In this demo, we take the following approach:\n",
    "#   * If a query has fewer documents, its Tensor will be padded\n",
    "#     appropriately.\n",
    "#   * If a query has more documents, we shuffle its list of\n",
    "#     documents and trim the list down to the prescribed list_size.\n",
    "_LIST_SIZE=10\n",
    "\n",
    "# The total number of features per query-document pair.\n",
    "# We set this number to the number of features in the MSLR-Web30K\n",
    "# dataset.\n",
    "_NUM_FEATURES=136\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE=32\n",
    "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OWN DATA\n",
    "\n",
    "# Store the paths to files containing training and test instances.\n",
    "# As noted above, we will assume the data is in the LibSVM format\n",
    "# and that the content of each file is sorted by query ID.\n",
    "_TRAIN_DATA_PATH_OWN=\"../data/interim/train_svm.txt\"\n",
    "_TEST_DATA_PATH_OWN=\"../data/interim/train_svm.txt\"\n",
    "\n",
    "# Define a loss function. To find a complete list of available\n",
    "# loss functions or to learn how to add your own custom function\n",
    "# please refer to the tensorflow_ranking.losses module.\n",
    "_LOSS=\"pairwise_logistic_loss\"\n",
    "\n",
    "# In the TF-Ranking framework, a training instance is represented\n",
    "# by a Tensor that contains features from a list of documents\n",
    "# associated with a single query. For simplicity, we fix the shape\n",
    "# of these Tensors to a maximum list size and call it \"list_size,\"\n",
    "# the maximum number of documents per query in the dataset.\n",
    "# In this demo, we take the following approach:\n",
    "#   * If a query has fewer documents, its Tensor will be padded\n",
    "#     appropriately.\n",
    "#   * If a query has more documents, we shuffle its list of\n",
    "#     documents and trim the list down to the prescribed list_size.\n",
    "_LIST_SIZE_OWN=10\n",
    "\n",
    "# The total number of features per query-document pair.\n",
    "# We set this number to the number of features in the MSLR-Web30K\n",
    "# dataset.\n",
    "_NUM_FEATURES_OWN=3\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE_OWN=32\n",
    "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(path):\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(path, _NUM_FEATURES, _LIST_SIZE),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
    "            for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.TensorShape([_LIST_SIZE])\n",
    "      )\n",
    "    )\n",
    "    train_dataset = train_dataset.shuffle(1000).repeat().batch(_BATCH_SIZE)\n",
    "    return train_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(_TRAIN_DATA_PATH, _NUM_FEATURES, _LIST_SIZE),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
    "            for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.TensorShape([_LIST_SIZE])\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_OWN = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(_TRAIN_DATA_PATH_OWN, _NUM_FEATURES_OWN, _LIST_SIZE_OWN),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES_OWN+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE_OWN, 1])\n",
    "            for k in range(1,_NUM_FEATURES_OWN+1)},\n",
    "          tf.TensorShape([_LIST_SIZE_OWN])\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_FEATURES_OWN=3\n",
    "_LIST_SIZE_OWN=10\n",
    "\n",
    "train_dataset_OWN = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(_TRAIN_DATA_PATH_OWN, _NUM_FEATURES_OWN, _LIST_SIZE_OWN),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES_OWN+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE_OWN, 1])\n",
    "            for k in range(1,_NUM_FEATURES_OWN+1)},\n",
    "          tf.TensorShape([_LIST_SIZE_OWN])\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({1: (10, 1), 2: (10, 1), 3: (10, 1), 4: (10, 1), 5: (10, 1), 6: (10, 1), 7: (10, 1), 8: (10, 1), 9: (10, 1), 10: (10, 1), 11: (10, 1), 12: (10, 1), 13: (10, 1), 14: (10, 1), 15: (10, 1), 16: (10, 1), 17: (10, 1), 18: (10, 1), 19: (10, 1), 20: (10, 1), 21: (10, 1), 22: (10, 1), 23: (10, 1), 24: (10, 1), 25: (10, 1), 26: (10, 1), 27: (10, 1), 28: (10, 1), 29: (10, 1), 30: (10, 1), 31: (10, 1), 32: (10, 1), 33: (10, 1), 34: (10, 1), 35: (10, 1), 36: (10, 1), 37: (10, 1), 38: (10, 1), 39: (10, 1), 40: (10, 1), 41: (10, 1), 42: (10, 1), 43: (10, 1), 44: (10, 1), 45: (10, 1), 46: (10, 1), 47: (10, 1), 48: (10, 1), 49: (10, 1), 50: (10, 1), 51: (10, 1), 52: (10, 1), 53: (10, 1), 54: (10, 1), 55: (10, 1), 56: (10, 1), 57: (10, 1), 58: (10, 1), 59: (10, 1), 60: (10, 1), 61: (10, 1), 62: (10, 1), 63: (10, 1), 64: (10, 1), 65: (10, 1), 66: (10, 1), 67: (10, 1), 68: (10, 1), 69: (10, 1), 70: (10, 1), 71: (10, 1), 72: (10, 1), 73: (10, 1), 74: (10, 1), 75: (10, 1), 76: (10, 1), 77: (10, 1), 78: (10, 1), 79: (10, 1), 80: (10, 1), 81: (10, 1), 82: (10, 1), 83: (10, 1), 84: (10, 1), 85: (10, 1), 86: (10, 1), 87: (10, 1), 88: (10, 1), 89: (10, 1), 90: (10, 1), 91: (10, 1), 92: (10, 1), 93: (10, 1), 94: (10, 1), 95: (10, 1), 96: (10, 1), 97: (10, 1), 98: (10, 1), 99: (10, 1), 100: (10, 1), 101: (10, 1), 102: (10, 1), 103: (10, 1), 104: (10, 1), 105: (10, 1), 106: (10, 1), 107: (10, 1), 108: (10, 1), 109: (10, 1), 110: (10, 1), 111: (10, 1), 112: (10, 1), 113: (10, 1), 114: (10, 1), 115: (10, 1), 116: (10, 1), 117: (10, 1), 118: (10, 1), 119: (10, 1), 120: (10, 1), 121: (10, 1), 122: (10, 1), 123: (10, 1), 124: (10, 1), 125: (10, 1), 126: (10, 1), 127: (10, 1), 128: (10, 1), 129: (10, 1), 130: (10, 1), 131: (10, 1), 132: (10, 1), 133: (10, 1), 134: (10, 1), 135: (10, 1), 136: (10, 1)}, (10,)), types: ({1: tf.float32, 2: tf.float32, 3: tf.float32, 4: tf.float32, 5: tf.float32, 6: tf.float32, 7: tf.float32, 8: tf.float32, 9: tf.float32, 10: tf.float32, 11: tf.float32, 12: tf.float32, 13: tf.float32, 14: tf.float32, 15: tf.float32, 16: tf.float32, 17: tf.float32, 18: tf.float32, 19: tf.float32, 20: tf.float32, 21: tf.float32, 22: tf.float32, 23: tf.float32, 24: tf.float32, 25: tf.float32, 26: tf.float32, 27: tf.float32, 28: tf.float32, 29: tf.float32, 30: tf.float32, 31: tf.float32, 32: tf.float32, 33: tf.float32, 34: tf.float32, 35: tf.float32, 36: tf.float32, 37: tf.float32, 38: tf.float32, 39: tf.float32, 40: tf.float32, 41: tf.float32, 42: tf.float32, 43: tf.float32, 44: tf.float32, 45: tf.float32, 46: tf.float32, 47: tf.float32, 48: tf.float32, 49: tf.float32, 50: tf.float32, 51: tf.float32, 52: tf.float32, 53: tf.float32, 54: tf.float32, 55: tf.float32, 56: tf.float32, 57: tf.float32, 58: tf.float32, 59: tf.float32, 60: tf.float32, 61: tf.float32, 62: tf.float32, 63: tf.float32, 64: tf.float32, 65: tf.float32, 66: tf.float32, 67: tf.float32, 68: tf.float32, 69: tf.float32, 70: tf.float32, 71: tf.float32, 72: tf.float32, 73: tf.float32, 74: tf.float32, 75: tf.float32, 76: tf.float32, 77: tf.float32, 78: tf.float32, 79: tf.float32, 80: tf.float32, 81: tf.float32, 82: tf.float32, 83: tf.float32, 84: tf.float32, 85: tf.float32, 86: tf.float32, 87: tf.float32, 88: tf.float32, 89: tf.float32, 90: tf.float32, 91: tf.float32, 92: tf.float32, 93: tf.float32, 94: tf.float32, 95: tf.float32, 96: tf.float32, 97: tf.float32, 98: tf.float32, 99: tf.float32, 100: tf.float32, 101: tf.float32, 102: tf.float32, 103: tf.float32, 104: tf.float32, 105: tf.float32, 106: tf.float32, 107: tf.float32, 108: tf.float32, 109: tf.float32, 110: tf.float32, 111: tf.float32, 112: tf.float32, 113: tf.float32, 114: tf.float32, 115: tf.float32, 116: tf.float32, 117: tf.float32, 118: tf.float32, 119: tf.float32, 120: tf.float32, 121: tf.float32, 122: tf.float32, 123: tf.float32, 124: tf.float32, 125: tf.float32, 126: tf.float32, 127: tf.float32, 128: tf.float32, 129: tf.float32, 130: tf.float32, 131: tf.float32, 132: tf.float32, 133: tf.float32, 134: tf.float32, 135: tf.float32, 136: tf.float32}, tf.float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({1: (10, 1), 2: (10, 1), 3: (10, 1)}, (10,)), types: ({1: tf.float32, 2: tf.float32, 3: tf.float32}, tf.float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_OWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/train_10.txt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_TRAIN_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "TypeError: 'NoneType' object does not support item assignment\nTraceback (most recent call last):\n\n  File \"/home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"/home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 449, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/data.py\", line 477, in inner_generator\n    num_features, list_size, doc_list)\n\n  File \"/home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/data.py\", line 424, in _libsvm_generate\n    features.get(feature_id)[idx, 0] = value\n\nTypeError: 'NoneType' object does not support item assignment\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-892e7bda79d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset_OWN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    632\u001b[0m     \"\"\"\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_gather_saveables_for_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1846\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: TypeError: 'NoneType' object does not support item assignment\nTraceback (most recent call last):\n\n  File \"/home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"/home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 449, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/data.py\", line 477, in inner_generator\n    num_features, list_size, doc_list)\n\n  File \"/home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/data.py\", line 424, in _libsvm_generate\n    features.get(feature_id)[idx, 0] = value\n\nTypeError: 'NoneType' object does not support item assignment\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "train_dataset_OWN.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.iterator_ops.EagerIterator at 0x7f43281e9470>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_OWN.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 qid:1 115:-0.617277 5:-0.109573 17:-0.763873 17:0.369905 68:-0.257625 125:0.570188 110:0.734393 136:-0.088640 135:-0.928225 133:-0.836065 13:-0.929646 63:-0.987756 131:-0.495995 84:-0.205543 107:0.956571 82:0.094344 69:0.111284 79:0.790958 42:-0.983709\n",
      "\n",
      "2 qid:1 75:-0.332035 84:0.926297 36:-0.071459 11:-0.522524 111:0.958885 98:-0.453638 62:0.350604 39:-0.775587 39:0.857645 105:-0.342500 88:0.094909 128:0.887842 57:-0.594232 48:-0.855241 135:0.210445 18:0.107854 81:0.769232 30:0.462099 13:0.876798\n",
      "\n",
      "2 qid:1 100:0.871419 118:-0.494900 29:-0.618690 114:-0.532670 27:0.928543 11:0.731187 80:-0.850780 72:0.967862 77:0.250605 86:-0.302680 132:0.807445 52:-0.217663 66:-0.464690 79:0.090635 39:-0.904325 133:-0.354280 45:0.394609 57:0.973102 64:0.959751\n",
      "\n",
      "1 qid:1 5:-0.329988 67:-0.945687 71:0.740200 132:0.644358 42:0.956237 102:-0.666758 117:0.771439 116:0.429287 10:-0.386381 51:-0.061408 31:0.376986 38:0.002500 103:0.388242 93:0.060171 26:-0.519465 94:0.180440 123:0.268493 131:0.514230 72:-0.344369\n",
      "\n",
      "2 qid:2 22:0.903892 107:0.994518 127:-0.859257 102:-0.987355 24:-0.892460 13:0.962668 24:0.468720 127:0.932992 88:0.815426 80:0.622727 32:-0.800153 124:0.938829 45:0.856294 28:-0.221539 116:-0.918705 44:0.985738 28:-0.759433 17:0.125985 86:0.432254\n",
      "\n",
      "1 qid:2 103:0.195366 64:-0.258920 79:0.109367 28:-0.774755 73:-0.508130 103:0.245937 73:0.981661 73:-0.374272 55:-0.152571 64:0.007325 111:0.876216 65:-0.840937 113:-0.368207 15:0.533191 93:0.634561 68:-0.655193 87:0.974906 39:-0.665351 73:0.190306\n",
      "\n",
      "1 qid:2 6:0.308428 53:-0.023596 7:0.195809 93:-0.614570 55:0.571983 44:-0.023181 66:0.389695 53:0.164865 43:0.097998 2:-0.147180 8:0.396087 72:0.268952 78:0.603235 25:0.794348 101:0.658756 69:0.120161 68:-0.085764 59:0.298807 86:-0.590223\n",
      "\n",
      "0 qid:2 1:0.062855 124:0.899163 81:0.386393 101:-0.746663 80:-0.575516 115:-0.852956 124:0.042402 136:-0.849485 70:-0.127201 47:0.312288 121:-0.752010 75:-0.257437 64:0.229110 68:-0.643063 1:-0.758929 87:0.795572 26:-0.661920 14:-0.600204 18:0.884136\n",
      "\n",
      "0 qid:3 80:0.046050 37:0.489725 87:0.214968 64:0.111221 51:-0.601959 81:-0.498310 130:-0.292314 136:0.584087 4:0.360063 55:-0.714872 61:0.685696 124:-0.246810 127:-0.111637 36:0.471735 64:0.577743 100:0.098417 91:-0.625811 39:-0.359663 83:0.304638\n",
      "\n",
      "2 qid:3 98:-0.302947 3:0.527615 71:0.242399 95:-0.020618 97:-0.929285 14:0.169941 104:0.331461 73:0.029981 13:0.281065 104:0.953007 60:0.887378 42:-0.828513 15:-0.647932 77:-0.765260 116:0.759152 31:-0.747571 19:0.551316 61:0.943195 113:-0.875075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fo = open(_TRAIN_DATA_PATH, 'r')\n",
    "\n",
    "for f in fo:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_feature_columns():\n",
    "    \"\"\"Returns the example feature columns.\"\"\"\n",
    "    feature_names = [\n",
    "      \"%d\" % (i + 1) for i in range(0, _NUM_FEATURES)]\n",
    "    return {\n",
    "      name: tf.feature_column.numeric_column(\n",
    "          name, shape=(1,), default_value=0.0) for name in feature_names\n",
    "    }\n",
    "\n",
    "def make_score_fn():\n",
    "    \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\n",
    "\n",
    "    def _score_fn(context_features, group_features, mode, params, config):\n",
    "        \"\"\"Defines the network to score a documents.\"\"\"\n",
    "        del params\n",
    "        del config\n",
    "        # Define input layer.\n",
    "        example_input = [\n",
    "            tf.layers.flatten(group_features[name])\n",
    "            for name in sorted(example_feature_columns())\n",
    "        ]\n",
    "        input_layer = tf.concat(example_input, 1)\n",
    "\n",
    "        cur_layer = input_layer\n",
    "        for i, layer_width in enumerate(int(d) for d in _HIDDEN_LAYER_DIMS):\n",
    "          cur_layer = tf.layers.dense(\n",
    "              cur_layer,\n",
    "              units=layer_width,\n",
    "              activation=\"tanh\")\n",
    "\n",
    "        logits = tf.layers.dense(cur_layer, units=1)\n",
    "        return logits\n",
    "\n",
    "    return _score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric_fns():\n",
    "    \"\"\"Returns a dict from name to metric functions.\n",
    "\n",
    "    This can be customized as follows. Care must be taken when handling padded\n",
    "    lists.\n",
    "\n",
    "    def _auc(labels, predictions, features):\n",
    "    is_label_valid = tf_reshape(tf.greater_equal(labels, 0.), [-1, 1])\n",
    "    clean_labels = tf.boolean_mask(tf.reshape(labels, [-1, 1], is_label_valid)\n",
    "    clean_pred = tf.boolean_maks(tf.reshape(predictions, [-1, 1], is_label_valid)\n",
    "    return tf.metrics.auc(clean_labels, tf.sigmoid(clean_pred), ...)\n",
    "    metric_fns[\"auc\"] = _auc\n",
    "\n",
    "    Returns:\n",
    "    A dict mapping from metric name to a metric function with above signature.\n",
    "    \"\"\"\n",
    "    metric_fns = {}\n",
    "    metric_fns.update({\n",
    "      \"metric/ndcg@%d\" % topn: tfr.metrics.make_ranking_metric_fn(\n",
    "          tfr.metrics.RankingMetricKey.NDCG, topn=topn)\n",
    "      for topn in [1, 3, 5, 10]\n",
    "    })\n",
    "\n",
    "    return metric_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(hparams):\n",
    "    \"\"\"Create a ranking estimator.\n",
    "\n",
    "    Args:\n",
    "    hparams: (tf.contrib.training.HParams) a hyperparameters object.\n",
    "\n",
    "    Returns:\n",
    "    tf.learn `Estimator`.\n",
    "    \"\"\"\n",
    "    def _train_op_fn(loss):\n",
    "        \"\"\"Defines train op used in ranking head.\"\"\"\n",
    "        return tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            learning_rate=hparams.learning_rate,\n",
    "            optimizer=\"Adagrad\")\n",
    "\n",
    "    ranking_head = tfr.head.create_ranking_head(\n",
    "      loss_fn=tfr.losses.make_loss_fn(_LOSS),\n",
    "      eval_metric_fns=eval_metric_fns(),\n",
    "      train_op_fn=_train_op_fn)\n",
    "\n",
    "    return tf.estimator.Estimator(\n",
    "      model_fn=tfr.model.make_groupwise_ranking_fn(\n",
    "          group_score_fn=make_score_fn(),\n",
    "          group_size=1,\n",
    "          transform_fn=None,\n",
    "          ranking_head=ranking_head),\n",
    "      params=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Hyperparameter Object\n",
    "hparams = tf.contrib.training.HParams(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxm75l6md\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpxm75l6md', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f43301ac208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "ranker = get_estimator(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n",
      "WARNING:tensorflow:From /home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/model.py:102: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-41-e6730bd6ae1e>:20: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-41-e6730bd6ae1e>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/model.py:268: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpxm75l6md/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1207455, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /tmp/tmpxm75l6md/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.25339997.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f43301ac978>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.train(input_fn=lambda: input_fn(_TRAIN_DATA_PATH), steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n",
      "WARNING:tensorflow:From /home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/metrics.py:152: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-15T12:59:10Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsdh0gfl2/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-15-12:59:16\n",
      "INFO:tensorflow:Saving dict for global step 10: global_step = 10, labels_mean = 0.7793427, logits_mean = 0.02370854, loss = 0.9659026, metric/ndcg@1 = 0.5187501, metric/ndcg@10 = 0.7910536, metric/ndcg@3 = 0.69284976, metric/ndcg@5 = 0.7547738\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /tmp/tmpsdh0gfl2/model.ckpt-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels_mean': 0.7793427,\n",
       " 'logits_mean': 0.02370854,\n",
       " 'loss': 0.9659026,\n",
       " 'metric/ndcg@1': 0.5187501,\n",
       " 'metric/ndcg@10': 0.7910536,\n",
       " 'metric/ndcg@3': 0.69284976,\n",
       " 'metric/ndcg@5': 0.7547738,\n",
       " 'global_step': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.evaluate(input_fn=lambda: input_fn(_TEST_DATA_PATH), steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct data like that\n",
    "\n",
    "target sid features_id:feature_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('../data/processed/df_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>click_time</th>\n",
       "      <th>click_mode</th>\n",
       "      <th>distance_plan</th>\n",
       "      <th>eta</th>\n",
       "      <th>price</th>\n",
       "      <th>transport_mode</th>\n",
       "      <th>plan_time</th>\n",
       "      <th>pid</th>\n",
       "      <th>req_time</th>\n",
       "      <th>o_long</th>\n",
       "      <th>o_lat</th>\n",
       "      <th>d_long</th>\n",
       "      <th>d_lat</th>\n",
       "      <th>distance_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2848914</td>\n",
       "      <td>2018-11-17 18:42:17</td>\n",
       "      <td>1</td>\n",
       "      <td>53156</td>\n",
       "      <td>6456</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>101804.0</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>116.36</td>\n",
       "      <td>40.07</td>\n",
       "      <td>116.00</td>\n",
       "      <td>40.35</td>\n",
       "      <td>43.656570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2848914</td>\n",
       "      <td>2018-11-17 18:42:17</td>\n",
       "      <td>1</td>\n",
       "      <td>48112</td>\n",
       "      <td>3535</td>\n",
       "      <td>700.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>101804.0</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>116.36</td>\n",
       "      <td>40.07</td>\n",
       "      <td>116.00</td>\n",
       "      <td>40.35</td>\n",
       "      <td>43.656570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2848914</td>\n",
       "      <td>2018-11-17 18:42:17</td>\n",
       "      <td>1</td>\n",
       "      <td>48112</td>\n",
       "      <td>3655</td>\n",
       "      <td>16500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>101804.0</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>116.36</td>\n",
       "      <td>40.07</td>\n",
       "      <td>116.00</td>\n",
       "      <td>40.35</td>\n",
       "      <td>43.656570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2848914</td>\n",
       "      <td>2018-11-17 18:42:17</td>\n",
       "      <td>1</td>\n",
       "      <td>51641</td>\n",
       "      <td>8871</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>101804.0</td>\n",
       "      <td>2018-11-17 12:56:15</td>\n",
       "      <td>116.36</td>\n",
       "      <td>40.07</td>\n",
       "      <td>116.00</td>\n",
       "      <td>40.35</td>\n",
       "      <td>43.656570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2629085</td>\n",
       "      <td>2018-10-12 16:28:13</td>\n",
       "      <td>3</td>\n",
       "      <td>13207</td>\n",
       "      <td>2790</td>\n",
       "      <td>400.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-10-12 16:27:55</td>\n",
       "      <td>203797.0</td>\n",
       "      <td>2018-10-12 16:27:55</td>\n",
       "      <td>116.35</td>\n",
       "      <td>40.08</td>\n",
       "      <td>116.33</td>\n",
       "      <td>40.03</td>\n",
       "      <td>5.808139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sid           click_time  click_mode  distance_plan   eta    price  \\\n",
       "0  2848914  2018-11-17 18:42:17           1          53156  6456    700.0   \n",
       "1  2848914  2018-11-17 18:42:17           1          48112  3535    700.0   \n",
       "2  2848914  2018-11-17 18:42:17           1          48112  3655  16500.0   \n",
       "3  2848914  2018-11-17 18:42:17           1          51641  8871   1200.0   \n",
       "4  2629085  2018-10-12 16:28:13           3          13207  2790    400.0   \n",
       "\n",
       "   transport_mode            plan_time       pid             req_time  o_long  \\\n",
       "0               1  2018-11-17 12:56:15  101804.0  2018-11-17 12:56:15  116.36   \n",
       "1               3  2018-11-17 12:56:15  101804.0  2018-11-17 12:56:15  116.36   \n",
       "2               4  2018-11-17 12:56:15  101804.0  2018-11-17 12:56:15  116.36   \n",
       "3               1  2018-11-17 12:56:15  101804.0  2018-11-17 12:56:15  116.36   \n",
       "4               9  2018-10-12 16:27:55  203797.0  2018-10-12 16:27:55  116.35   \n",
       "\n",
       "   o_lat  d_long  d_lat  distance_query  \n",
       "0  40.07  116.00  40.35       43.656570  \n",
       "1  40.07  116.00  40.35       43.656570  \n",
       "2  40.07  116.00  40.35       43.656570  \n",
       "3  40.07  116.00  40.35       43.656570  \n",
       "4  40.08  116.33  40.03        5.808139  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_train[[\"sid\", \"click_mode\", \"distance_plan\", \"eta\", \"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>click_mode</th>\n",
       "      <th>distance_plan</th>\n",
       "      <th>eta</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2848914</td>\n",
       "      <td>1</td>\n",
       "      <td>53156</td>\n",
       "      <td>6456</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2848914</td>\n",
       "      <td>1</td>\n",
       "      <td>48112</td>\n",
       "      <td>3535</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2848914</td>\n",
       "      <td>1</td>\n",
       "      <td>48112</td>\n",
       "      <td>3655</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2848914</td>\n",
       "      <td>1</td>\n",
       "      <td>51641</td>\n",
       "      <td>8871</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2629085</td>\n",
       "      <td>3</td>\n",
       "      <td>13207</td>\n",
       "      <td>2790</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sid  click_mode  distance_plan   eta    price\n",
       "0  2848914           1          53156  6456    700.0\n",
       "1  2848914           1          48112  3535    700.0\n",
       "2  2848914           1          48112  3655  16500.0\n",
       "3  2848914           1          51641  8871   1200.0\n",
       "4  2629085           3          13207  2790    400.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.columns.get_loc(\"distance_plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm = pd.DataFrame(columns=['target', 'sid', 'distance_plan', 'eta', 'price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.target = df_X.click_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.sid = df_X.sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.distance_plan = df_X.distance_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.distance_plan = '1:' + df_svm.distance_plan.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.eta = df_X.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.eta = '2:' + df_svm.eta.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.price = df_X.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.price = '3:' + df_svm.price.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sid</th>\n",
       "      <th>distance_plan</th>\n",
       "      <th>eta</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2848914</td>\n",
       "      <td>1:53156</td>\n",
       "      <td>2:6456</td>\n",
       "      <td>3:700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2848914</td>\n",
       "      <td>1:48112</td>\n",
       "      <td>2:3535</td>\n",
       "      <td>3:700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2848914</td>\n",
       "      <td>1:48112</td>\n",
       "      <td>2:3655</td>\n",
       "      <td>3:16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2848914</td>\n",
       "      <td>1:51641</td>\n",
       "      <td>2:8871</td>\n",
       "      <td>3:1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2629085</td>\n",
       "      <td>1:13207</td>\n",
       "      <td>2:2790</td>\n",
       "      <td>3:400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target      sid distance_plan     eta      price\n",
       "0       1  2848914       1:53156  2:6456    3:700.0\n",
       "1       1  2848914       1:48112  2:3535    3:700.0\n",
       "2       1  2848914       1:48112  2:3655  3:16500.0\n",
       "3       1  2848914       1:51641  2:8871   3:1200.0\n",
       "4       3  2629085       1:13207  2:2790    3:400.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.sort_values(\"sid\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svm.to_csv('../data/interim/train_tfr.txt', header=None, index=None, sep=' ', mode='a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With own training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import dump_svmlight_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_X[[\"distance_plan\", \"eta\", \"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_svm[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y=np.arange(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=\"../data/interim/train_svm.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = df_svm.sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = query_id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X=X, y=y, f=f, query_id=query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 qid:10 0:53156 1:6456 2:700\n",
      "\n",
      "1 qid:10 0:48112 1:3535 2:700\n",
      "\n",
      "2 qid:10 0:48112 1:3655 2:16500\n",
      "\n",
      "3 qid:10 0:51641 1:8871 2:1200\n",
      "\n",
      "4 qid:10 0:13207 1:2790 2:400\n",
      "\n",
      "5 qid:10 0:8175 1:1656 2:700\n",
      "\n",
      "6 qid:21 0:8175 1:1776 2:2700\n",
      "\n",
      "7 qid:21 0:9620 1:2424 2:1600\n",
      "\n",
      "8 qid:21 0:8079 1:2443 2:700\n",
      "\n",
      "9 qid:25 0:13428 1:3777 2:800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fo = open(f, 'r')\n",
    "\n",
    "for line in fo:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_evaluation_set():\n",
    "    import numpy as np\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'SepalLength': array([6.4, 5. ]),\n",
       "  'SepalWidth': array([2.8, 2.3]),\n",
       "  'PetalLength': array([5.6, 3.3]),\n",
       "  'PetalWidth': array([2.2, 1. ])},\n",
       " array([2, 1]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_evaluation_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow_ranking.python.data.libsvm_generator.<locals>.inner_generator()>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfr.data.libsvm_generator(path, _NUM_FEATURES, _LIST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "  tfr.data.libsvm_generator(path, _NUM_FEATURES, _LIST_SIZE),\n",
    "  output_types=(\n",
    "      {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
    "      tf.float32\n",
    "  ),\n",
    "  output_shapes=(\n",
    "      {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
    "        for k in range(1,_NUM_FEATURES+1)},\n",
    "      tf.TensorShape([_LIST_SIZE])\n",
    "  )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(1000).repeat().batch(_BATCH_SIZE)\n",
    "return train_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_text_line(path):\n",
    "    ds = tf.data.TextLineDataset(path)\n",
    "    ds.shuffle(1000).repeat().batch(_BATCH_SIZE)\n",
    "    return ds.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (?,), types: tf.string>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = _TRAIN_DATA_PATH_OWN\n",
    "ds = tf.data.TextLineDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (?,), types: tf.string>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shuffle(1000).repeat().batch(_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = ds.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-56d0550516ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1078\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    for i in range(3):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_libsvm_data(path, list_size):\n",
    "    \"\"\"Returns features and labels in numpy.array.\"\"\"\n",
    "    import numpy as np\n",
    "    import six\n",
    "\n",
    "    def _parse_line(line):\n",
    "        \"\"\"Parses a single line in LibSVM format.\"\"\"\n",
    "        tokens = line.split(\"#\")[0].split()\n",
    "        assert len(tokens) >= 2, \"Ill-formatted line: {}\".format(line)\n",
    "        label = float(tokens[0])\n",
    "        qid = tokens[1]\n",
    "        kv_pairs = [kv.split(\":\") for kv in tokens[2:]]\n",
    "        features = {k: float(v) for (k, v) in kv_pairs}\n",
    "        return qid, features, label\n",
    "\n",
    "    tf.compat.v1.logging.info(\"Loading data from {}\".format(path))\n",
    "\n",
    "    # The 0-based index assigned to a query.\n",
    "    qid_to_index = {}\n",
    "    # The number of docs seen so far for a query.\n",
    "    qid_to_ndoc = {}\n",
    "    # Each feature is mapped an array with [num_queries, list_size, 1]. Label has\n",
    "    # a shape of [num_queries, list_size]. We use list for each of them due to the\n",
    "    # unknown number of quries.\n",
    "    feature_map = {k: [] for k in example_feature_columns()}\n",
    "    label_list = []\n",
    "    total_docs = 0\n",
    "    discarded_docs = 0\n",
    "    with open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            qid, features, label = _parse_line(line)\n",
    "            if qid not in qid_to_index:\n",
    "                # Create index and allocate space for a new query.\n",
    "                qid_to_index[qid] = len(qid_to_index)\n",
    "                qid_to_ndoc[qid] = 0\n",
    "                for k in feature_map:\n",
    "                    feature_map[k].append(np.zeros([list_size, 1], dtype=np.float32))\n",
    "                label_list.append(np.ones([list_size], dtype=np.float32) * -1.)\n",
    "            total_docs += 1\n",
    "            batch_idx = qid_to_index[qid]\n",
    "            doc_idx = qid_to_ndoc[qid]\n",
    "            qid_to_ndoc[qid] += 1\n",
    "            # Keep the first 'list_size' docs only.\n",
    "            if doc_idx >= list_size:\n",
    "                discarded_docs += 1\n",
    "                continue\n",
    "            for k, v in six.iteritems(features):\n",
    "                assert k in feature_map, \"Key {} not found in features.\".format(k)\n",
    "                feature_map[k][batch_idx][doc_idx, 0] = v\n",
    "            label_list[batch_idx][doc_idx] = label\n",
    "\n",
    "    tf.compat.v1.logging.info(\"Number of queries: {}\".format(len(qid_to_index)))\n",
    "    tf.compat.v1.logging.info(\n",
    "        \"Number of documents in total: {}\".format(total_docs))\n",
    "    tf.compat.v1.logging.info(\n",
    "        \"Number of documents discarded: {}\".format(discarded_docs))\n",
    "\n",
    "    # Convert everything to np.array.\n",
    "    for k in feature_map:\n",
    "        feature_map[k] = np.array(feature_map[k])\n",
    "    return feature_map, np.array(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  EXAMPLE\n",
    "\n",
    "# Store the paths to files containing training and test instances.\n",
    "# As noted above, we will assume the data is in the LibSVM format\n",
    "# and that the content of each file is sorted by query ID.\n",
    "_TRAIN_DATA_PATH=\"/tmp/train.txt\"\n",
    "_TEST_DATA_PATH=\"/tmp/test.txt\"\n",
    "path = _TRAIN_DATA_PATH\n",
    "\n",
    "# Define a loss function. To find a complete list of available\n",
    "# loss functions or to learn how to add your own custom function\n",
    "# please refer to the tensorflow_ranking.losses module.\n",
    "_LOSS=\"pairwise_logistic_loss\"\n",
    "\n",
    "# In the TF-Ranking framework, a training instance is represented\n",
    "# by a Tensor that contains features from a list of documents\n",
    "# associated with a single query. For simplicity, we fix the shape\n",
    "# of these Tensors to a maximum list size and call it \"list_size,\"\n",
    "# the maximum number of documents per query in the dataset.\n",
    "# In this demo, we take the following approach:\n",
    "#   * If a query has fewer documents, its Tensor will be padded\n",
    "#     appropriately.\n",
    "#   * If a query has more documents, we shuffle its list of\n",
    "#     documents and trim the list down to the prescribed list_size.\n",
    "_LIST_SIZE=10\n",
    "\n",
    "# The total number of features per query-document pair.\n",
    "# We set this number to the number of features in the MSLR-Web30K\n",
    "# dataset.\n",
    "_NUM_FEATURES=136\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE=10\n",
    "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-88-a6c391f793b7>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-88-a6c391f793b7>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print(qid not in qid_to_index)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def _parse_line(line):\n",
    "    \"\"\"Parses a single line in LibSVM format.\"\"\"\n",
    "    tokens = line.split(\"#\")[0].split()\n",
    "    assert len(tokens) >= 2, \"Ill-formatted line: {}\".format(line)\n",
    "    label = float(tokens[0])\n",
    "    qid = tokens[1]\n",
    "    kv_pairs = [kv.split(\":\") for kv in tokens[2:]]\n",
    "    features = {k: float(v) for (k, v) in kv_pairs}\n",
    "    return qid, features, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_feature_columns():\n",
    "    \"\"\"Returns the example feature columns.\"\"\"\n",
    "    feature_names = [\"{}\".format(i + 1) for i in range(_NUM_FEATURES_OWN)]\n",
    "    return {\n",
    "      name: tf.feature_column.numeric_column(\n",
    "          name, shape=(1,), default_value=0.0) for name in feature_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 -0.617277\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Key 115 not found in features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-f92d0fc967da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Key {} not found in features.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mfeature_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Key 115 not found in features."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "list_size = _LIST_SIZE\n",
    "# The 0-based index assigned to a query.\n",
    "qid_to_index = {}\n",
    "# The number of docs seen so far for a query.\n",
    "qid_to_ndoc = {}\n",
    "# Each feature is mapped an array with [num_queries, list_size, 1]. Label has\n",
    "# a shape of [num_queries, list_size]. We use list for each of them due to the\n",
    "# unknown number of quries.\n",
    "feature_map = {k: [] for k in example_feature_columns()}\n",
    "label_list = []\n",
    "total_docs = 0\n",
    "discarded_docs = 0\n",
    "with open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            qid, features, label = _parse_line(line)\n",
    "            if qid not in qid_to_index:\n",
    "                qid_to_index[qid] = len(qid_to_index)\n",
    "                qid_to_ndoc[qid] = 0\n",
    "            for k in feature_map:\n",
    "                # Create feature map with size of list size (tensor later?)\n",
    "                feature_map[k].append(np.zeros([list_size, 1], dtype=np.float32))\n",
    "            # Create label_list in size of list_size (columns) with the value -1\n",
    "            label_list.append(np.ones([list_size], dtype=np.float32) * -1.)\n",
    "            total_docs += 1\n",
    "            batch_idx = qid_to_index[qid]\n",
    "            doc_idx = qid_to_ndoc[qid]\n",
    "            # Aggregation count? \n",
    "            qid_to_ndoc[qid] += 1\n",
    "            # Keep the first 'list_size' docs only.\n",
    "            if doc_idx >= list_size:\n",
    "                print(doc_idx)\n",
    "                print(list_size)\n",
    "                discarded_docs += 1\n",
    "                continue\n",
    "            for k, v in six.iteritems(features):\n",
    "                print(k, v)\n",
    "                assert k in feature_map, \"Key {} not found in features.\".format(k)\n",
    "                feature_map[k][batch_idx][doc_idx, 0] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /tmp/tmp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'train_path' is defined twice. First from /home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: Input file path used for training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-95ee1f9eaf0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_ranking\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/home/sandro/repo/ranking/tensorflow_ranking/examples/data/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Input file path used for training.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vali_path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/home/sandro/repo/ranking/tensorflow_ranking/examples/data/vali.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Input file path used for validation.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/home/sandro/repo/ranking/tensorflow_ranking/examples/data/test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Input file path used for testing.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'train_path' is defined twice. First from /home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: Input file path used for training."
     ]
    }
   ],
   "source": [
    "\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "flags.DEFINE_string(\"train_path\", \"/home/sandro/repo/ranking/tensorflow_ranking/examples/data/train.txt\", \"Input file path used for training.\")\n",
    "flags.DEFINE_string(\"vali_path\", \"/home/sandro/repo/ranking/tensorflow_ranking/examples/data/vali.txt\", \"Input file path used for validation.\")\n",
    "flags.DEFINE_string(\"test_path\", \"/home/sandro/repo/ranking/tensorflow_ranking/examples/data/test.txt\", \"Input file path used for testing.\")\n",
    "flags.DEFINE_string(\"output_dir\", \"/tmp/tmp_out\", \"Output directory for models.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"The batch size for training.\")\n",
    "flags.DEFINE_integer(\"num_train_steps\", 100000, \"Number of steps for training.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate for optimizer.\")\n",
    "flags.DEFINE_float(\"dropout_rate\", 0.5, \"The dropout rate before output layer.\")\n",
    "flags.DEFINE_list(\"hidden_layer_dims\", [\"256\", \"128\", \"64\"],\n",
    "                  \"Sizes for hidden layers.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_features\", 136, \"Number of features per document.\")\n",
    "flags.DEFINE_integer(\"list_size\", 100, \"List size used for training.\")\n",
    "flags.DEFINE_integer(\"group_size\", 1, \"Group size used in score function.\")\n",
    "\n",
    "flags.DEFINE_string(\"loss\", \"pairwise_logistic_loss\",\n",
    "                    \"The RankingLossKey for loss function.\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class IteratorInitializerHook(tf.estimator.SessionRunHook):\n",
    "  \"\"\"Hook to initialize data iterator after session is created.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super(IteratorInitializerHook, self).__init__()\n",
    "    self.iterator_initializer_fn = None\n",
    "\n",
    "  def after_create_session(self, session, coord):\n",
    "    \"\"\"Initialize the iterator after the session has been created.\"\"\"\n",
    "    del coord\n",
    "    self.iterator_initializer_fn(session)\n",
    "\n",
    "\n",
    "def example_feature_columns():\n",
    "  \"\"\"Returns the example feature columns.\"\"\"\n",
    "  feature_names = [\"{}\".format(i + 1) for i in range(FLAGS.num_features)]\n",
    "  return {\n",
    "      name: tf.feature_column.numeric_column(\n",
    "          name, shape=(1,), default_value=0.0) for name in feature_names\n",
    "  }\n",
    "\n",
    "\n",
    "def load_libsvm_data(path, list_size):\n",
    "  \"\"\"Returns features and labels in numpy.array.\"\"\"\n",
    "\n",
    "  def _parse_line(line):\n",
    "    \"\"\"Parses a single line in LibSVM format.\"\"\"\n",
    "    tokens = line.split(\"#\")[0].split()\n",
    "    assert len(tokens) >= 2, \"Ill-formatted line: {}\".format(line)\n",
    "    label = float(tokens[0])\n",
    "    qid = tokens[1]\n",
    "    kv_pairs = [kv.split(\":\") for kv in tokens[2:]]\n",
    "    features = {k: float(v) for (k, v) in kv_pairs}\n",
    "    return qid, features, label\n",
    "\n",
    "  tf.compat.v1.logging.info(\"Loading data from {}\".format(path))\n",
    "\n",
    "  # The 0-based index assigned to a query.\n",
    "  qid_to_index = {}\n",
    "  # The number of docs seen so far for a query.\n",
    "  qid_to_ndoc = {}\n",
    "  # Each feature is mapped an array with [num_queries, list_size, 1]. Label has\n",
    "  # a shape of [num_queries, list_size]. We use list for each of them due to the\n",
    "  # unknown number of quries.\n",
    "  feature_map = {k: [] for k in example_feature_columns()}\n",
    "  label_list = []\n",
    "  total_docs = 0\n",
    "  discarded_docs = 0\n",
    "  with open(path, \"rt\") as f:\n",
    "    for line in f:\n",
    "      qid, features, label = _parse_line(line)\n",
    "      if qid not in qid_to_index:\n",
    "        # Create index and allocate space for a new query.\n",
    "        qid_to_index[qid] = len(qid_to_index)\n",
    "        qid_to_ndoc[qid] = 0\n",
    "        for k in feature_map:\n",
    "          feature_map[k].append(np.zeros([list_size, 1], dtype=np.float32))\n",
    "        label_list.append(np.ones([list_size], dtype=np.float32) * -1.)\n",
    "      total_docs += 1\n",
    "      batch_idx = qid_to_index[qid]\n",
    "      doc_idx = qid_to_ndoc[qid]\n",
    "      qid_to_ndoc[qid] += 1\n",
    "      # Keep the first 'list_size' docs only.\n",
    "      if doc_idx >= list_size:\n",
    "        discarded_docs += 1\n",
    "        continue\n",
    "      for k, v in six.iteritems(features):\n",
    "        print(k, v)\n",
    "        assert k in feature_map, \"Key {} not found in features.\".format(k)\n",
    "        feature_map[k][batch_idx][doc_idx, 0] = v\n",
    "      label_list[batch_idx][doc_idx] = label\n",
    "    print(feature_map)\n",
    "    return null\n",
    "  tf.compat.v1.logging.info(\"Number of queries: {}\".format(len(qid_to_index)))\n",
    "  tf.compat.v1.logging.info(\n",
    "      \"Number of documents in total: {}\".format(total_docs))\n",
    "  tf.compat.v1.logging.info(\n",
    "      \"Number of documents discarded: {}\".format(discarded_docs))\n",
    "\n",
    "  # Convert everything to np.array.\n",
    "  for k in feature_map:\n",
    "    feature_map[k] = np.array(feature_map[k])\n",
    "  return feature_map, np.array(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
