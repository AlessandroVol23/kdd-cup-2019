{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general recipe is a short list of four main steps:\n",
    "\n",
    "1.   Compose a function to **read** input data and prepare a Tensorflow Dataset;\n",
    "2.   Define a **scoring** function that, given a (set of) query-document feature vector(s), produces a score indicating the query's level of relevance to the document;\n",
    "3.   Create a **loss** function that measures how far off the produced scores from step (2) are from the ground truth; and,\n",
    "4.   Define evaluation **metrics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import pandas as pd\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the paths to files containing training and test instances.\n",
    "# As noted above, we will assume the data is in the LibSVM format\n",
    "# and that the content of each file is sorted by query ID.\n",
    "\n",
    "_TRAIN_DATA_PATH=\"../data/interim/df_train_test.libsvm\"\n",
    "_TEST_DATA_PATH=\"../data/interim/df_train_test.libsvm\"\n",
    "\n",
    "# Define a loss function. To find a complete list of available\n",
    "# loss functions or to learn how to add your own custom function\n",
    "# please refer to the tensorflow_ranking.losses module.\n",
    "_LOSS=\"pairwise_logistic_loss\"\n",
    "\n",
    "# In the TF-Ranking framework, a training instance is represented\n",
    "# by a Tensor that contains features from a list of documents\n",
    "# associated with a single query. For simplicity, we fix the shape\n",
    "# of these Tensors to a maximum list size and call it \"list_size,\"\n",
    "# the maximum number of documents per query in the dataset.\n",
    "# In this demo, we take the following approach:\n",
    "#   * If a query has fewer documents, its Tensor will be padded\n",
    "#     appropriately.\n",
    "#   * If a query has more documents, we shuffle its list of\n",
    "#     documents and trim the list down to the prescribed list_size.\n",
    "_LIST_SIZE=100\n",
    "\n",
    "# The total number of features per query-document pair.\n",
    "# We set this number to the number of features in the MSLR-Web30K\n",
    "# dataset.\n",
    "_NUM_FEATURES=4\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE=100\n",
    "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]\n",
    "\n",
    "#_OUT_DIR = \"../models/tfranking/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to read in data and form tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out train and test dataset is in the lib svm format which is normally used for Support Vector Machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 qid:2558352 1:5 2:4694 3:4228 4:700\n",
      "\n",
      "0 qid:2558352 1:3 2:5113 3:604 4:700\n",
      "\n",
      "0 qid:2558352 1:6 2:4703 3:1420 4:700\n",
      "\n",
      "1 qid:2558352 1:1 2:5380 3:1930 4:200\n",
      "\n",
      "0 qid:2558353 1:4 2:6033 3:1099 4:2300\n",
      "\n",
      "1 qid:2558353 1:2 2:4381 3:1612 4:300\n",
      "\n",
      "0 qid:2558353 1:9 2:4382 3:1370 4:300\n",
      "\n",
      "0 qid:2558353 1:5 2:4045 3:3532 4:700\n",
      "\n",
      "0 qid:2558353 1:3 2:6033 3:799 4:700\n",
      "\n",
      "0 qid:2558353 1:6 2:4078 3:1231 4:700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fo = open(_TRAIN_DATA_PATH)\n",
    "i=0\n",
    "for f in fo:\n",
    "    if i != 10:\n",
    "        print(f)\n",
    "    else:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example they first number shows the target\n",
    "\n",
    "1 = Important \n",
    "\n",
    "0 = Not important\n",
    "\n",
    "qid: Describes which lines belong together\n",
    "\n",
    "E.g. query 10 had 6 suggestions for plans and just one is important. Here we would take the suggested transport mode from this plan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pipeline\n",
    "\n",
    "The first step to construct an input pipeline that reads your dataset and produces a `tensorflow.data.Dataset` object. In this example, we will invoke a LibSVM parser that is included in the `tensorflow_ranking.data` module to generate a `Dataset` from a given file.\n",
    "\n",
    "We parameterize this function by a `path` argument so that the function can be used to read both training and test data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Read tf DataSet\n",
    " \n",
    " Dic for feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(path):\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(path, _NUM_FEATURES, _LIST_SIZE),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
    "            for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.TensorShape([_LIST_SIZE])\n",
    "      )\n",
    "    )\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(1000).repeat().batch(_BATCH_SIZE)\n",
    "    return train_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({1: (100, 1), 2: (100, 1), 3: (100, 1), 4: (100, 1)}, (100,)), types: ({1: tf.float32, 2: tf.float32, 3: tf.float32, 4: tf.float32}, tf.float32)>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(_TRAIN_DATA_PATH, _NUM_FEATURES, _LIST_SIZE),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
    "            for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.TensorShape([_LIST_SIZE])\n",
    "      )\n",
    "    )\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=train_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': <tf.Tensor 'IteratorGetNext_1:0' shape=(100, 1) dtype=float32>,\n",
       " '2': <tf.Tensor 'IteratorGetNext_1:1' shape=(100, 1) dtype=float32>,\n",
       " '3': <tf.Tensor 'IteratorGetNext_1:2' shape=(100, 1) dtype=float32>,\n",
       " '4': <tf.Tensor 'IteratorGetNext_1:3' shape=(100, 1) dtype=float32>}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we turn to the scoring function which is arguably at the heart of a TF Ranking model. The idea is to compute a relevance score for a (set of) query-document pair(s). The TF-Ranking model will use training data to learn this function.\n",
    "\n",
    "Here we formulate a scoring function using a feed forward network. The function takes the features of a single example (i.e., query-document pair) and produces a relevance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_feature_columns():\n",
    "    \"\"\"Returns the example feature columns.\"\"\"\n",
    "    feature_names = [\n",
    "      \"%d\" % (i + 1) for i in range(0, _NUM_FEATURES)\n",
    "    ]\n",
    "    return {\n",
    "      name: tf.feature_column.numeric_column(\n",
    "          name, shape=(1,), default_value=0.0) for name in feature_names\n",
    "    }\n",
    "\n",
    "def make_score_fn(mode):\n",
    "        \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\n",
    "        \n",
    "        def _score_fn(context_features, group_features, mode, params, config):\n",
    "            \"\"\"Defines the network to score a documents.\"\"\"\n",
    "            del params\n",
    "            del config\n",
    "            # Define input layer.\n",
    "            example_input = [\n",
    "                tf.layers.flatten(group_features[name])\n",
    "                for name in sorted(example_feature_columns())\n",
    "            ]\n",
    "            print(\"MAKE SCORE FUNCTION:\")\n",
    "            print(example_input)\n",
    "            input_layer = tf.concat(example_input, 1)\n",
    "\n",
    "            cur_layer = input_layer\n",
    "            for i, layer_width in enumerate(int(d) for d in _HIDDEN_LAYER_DIMS):\n",
    "                cur_layer = tf.layers.dense(\n",
    "                  cur_layer,\n",
    "                  units=layer_width,\n",
    "                  activation=\"tanh\")\n",
    "\n",
    "            logits = tf.layers.dense(cur_layer, units=1)\n",
    "            return logits\n",
    "        return _score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric_fns():\n",
    "    \"\"\"Returns a dict from name to metric functions.\n",
    "\n",
    "    This can be customized as follows. Care must be taken when handling padded\n",
    "    lists.\n",
    "\n",
    "    def _auc(labels, predictions, features):\n",
    "    is_label_valid = tf_reshape(tf.greater_equal(labels, 0.), [-1, 1])\n",
    "    clean_labels = tf.boolean_mask(tf.reshape(labels, [-1, 1], is_label_valid)\n",
    "    clean_pred = tf.boolean_maks(tf.reshape(predictions, [-1, 1], is_label_valid)\n",
    "    return tf.metrics.auc(clean_labels, tf.sigmoid(clean_pred), ...)\n",
    "    metric_fns[\"auc\"] = _auc\n",
    "\n",
    "    Returns:\n",
    "    A dict mapping from metric name to a metric function with above signature.\n",
    "    \"\"\"\n",
    "    metric_fns = {}\n",
    "    metric_fns.update({\n",
    "      \"metric/ndcg@%d\" % topn: tfr.metrics.make_ranking_metric_fn(\n",
    "          tfr.metrics.RankingMetricKey.NDCG, topn=topn)\n",
    "      for topn in [1, 3, 5, 10]\n",
    "    })\n",
    "\n",
    "    return metric_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(hparams, mode):\n",
    "    \"\"\"Create a ranking estimator.\n",
    "\n",
    "    Args:\n",
    "    hparams: (tf.contrib.training.HParams) a hyperparameters object.\n",
    "\n",
    "    Returns:\n",
    "    tf.learn `Estimator`.\n",
    "    \"\"\"\n",
    "    def _train_op_fn(loss):\n",
    "        \"\"\"Defines train op used in ranking head.\"\"\"\n",
    "        return tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            learning_rate=hparams.learning_rate,\n",
    "            optimizer=\"Adagrad\")\n",
    "\n",
    "    ranking_head = tfr.head.create_ranking_head(\n",
    "      loss_fn=tfr.losses.make_loss_fn(_LOSS),\n",
    "      eval_metric_fns=eval_metric_fns(),\n",
    "      train_op_fn=_train_op_fn)\n",
    "\n",
    "    return tf.estimator.Estimator(\n",
    "      model_fn=tfr.model.make_groupwise_ranking_fn(\n",
    "          group_score_fn=make_score_fn(mode),\n",
    "          group_size=1,\n",
    "          transform_fn=None,\n",
    "          ranking_head=ranking_head),\n",
    "        params=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpvvsu96z2\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpvvsu96z2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f67fd655048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(learning_rate=0.05)\n",
    "ranker = get_estimator(hparams, tf.estimator.ModeKeys.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n",
      "MAKE SCORE FUNCTION:\n",
      "[<tf.Tensor 'groupwise_dnn_v2/group_score/flatten/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_1/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_2/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_3/Reshape:0' shape=(?, 1) dtype=float32>]\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpvvsu96z2/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.75400585, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /tmp/tmpvvsu96z2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6130944.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f67fd6a2ba8>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.train(input_fn=lambda: input_fn(_TRAIN_DATA_PATH), steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f67fd6a2ba8>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltr_to_submission(df, ranker, path):\n",
    "    import numpy as np\n",
    "    preds = ranker.predict(input_fn=lambda: input_fn(path))\n",
    "    import itertools\n",
    "    preds_slice = itertools.islice(preds, _LIST_SIZE) # grab \n",
    "    count=0\n",
    "    a = np.zeros((len(df), _LIST_SIZE))\n",
    "\n",
    "    for i in preds_slice:\n",
    "        a[count]=i\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "    test_X = df[[\n",
    "       'sid',\n",
    "        'transport_mode',\n",
    "        'distance_plan',\n",
    "        'eta', \n",
    "        'price'\n",
    "    ]]\n",
    "    \n",
    "    # Assign prediction vals to df\n",
    "    test_X = test_X.assign(yhat = a[:,0])\n",
    "    \n",
    "    df_end = pd.DataFrame(columns=['yhat'], index=df.sid.unique())\n",
    "\n",
    "    df_end = test_X.sort_values(['sid', 'yhat'], ascending=False).groupby('sid').first()[[\n",
    "        'yhat', 'transport_mode'\n",
    "    ]]\n",
    "    \n",
    "    return df_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_train = pd.read_pickle(\"../data/interim/df_train_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test = pd.read_pickle(\"../data/interim/df_train_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n",
      "MAKE SCORE FUNCTION:\n",
      "[<tf.Tensor 'groupwise_dnn_v2/group_score/flatten/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_1/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_2/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_3/Reshape:0' shape=(?, 1) dtype=float32>]\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpvvsu96z2/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "df_preds = ltr_to_submission(df_train_train, ranker, '../data/interim/df_train_train.libsvm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3252145410735725"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_train_train.groupby(\"sid\").first()['click_mode'], df_preds.transport_mode, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end = test_X[msk][['sid', 'transport_mode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143113     0.509255\n",
       "143110     0.509255\n",
       "143112     0.509255\n",
       "143109     0.509255\n",
       "459059     0.509255\n",
       "459057     0.509255\n",
       "459061     0.509255\n",
       "459062     0.509255\n",
       "459058     0.509255\n",
       "459060     0.509255\n",
       "909545     0.509255\n",
       "909546     0.509255\n",
       "909548     0.509255\n",
       "909547     0.509255\n",
       "241776    -1.318469\n",
       "241774    -1.318469\n",
       "241775    -1.318469\n",
       "241773    -1.318469\n",
       "241777    -1.318469\n",
       "1270163    0.509255\n",
       "1270164    0.509255\n",
       "1270165    0.509255\n",
       "1270168    0.509255\n",
       "1270167    0.509255\n",
       "1270166    0.509255\n",
       "316955     0.509255\n",
       "316959     0.509255\n",
       "316957     0.509255\n",
       "316958     0.509255\n",
       "316956     0.509255\n",
       "             ...   \n",
       "1525598    0.000000\n",
       "1525603    0.000000\n",
       "1525602    0.000000\n",
       "1525599    0.000000\n",
       "1525600    0.000000\n",
       "45680      0.000000\n",
       "45682      0.000000\n",
       "45681      0.000000\n",
       "45678      0.000000\n",
       "45679      0.000000\n",
       "45683      0.000000\n",
       "1449538    0.000000\n",
       "1449537    0.000000\n",
       "1449539    0.000000\n",
       "1449535    0.000000\n",
       "1449536    0.000000\n",
       "1326911    0.000000\n",
       "1326910    0.000000\n",
       "1326912    0.000000\n",
       "1326909    0.000000\n",
       "1326908    0.000000\n",
       "17801      0.000000\n",
       "17802      0.000000\n",
       "17800      0.000000\n",
       "17798      0.000000\n",
       "17799      0.000000\n",
       "861591     0.000000\n",
       "861590     0.000000\n",
       "861592     0.000000\n",
       "861593     0.000000\n",
       "Name: yhat, Length: 601059, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.groupby('sid', sort=False)['yhat'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['new']=test_X.groupby('sid')['yhat'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129542, 6)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.sort_values(['sid', 'yhat'], ascending=False).groupby('sid').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600983, 2)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_end.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
