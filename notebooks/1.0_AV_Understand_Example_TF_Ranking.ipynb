{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general recipe is a short list of four main steps:\n",
    "\n",
    "1.   Compose a function to **read** input data and prepare a Tensorflow Dataset;\n",
    "2.   Define a **scoring** function that, given a (set of) query-document feature vector(s), produces a score indicating the query's level of relevance to the document;\n",
    "3.   Create a **loss** function that measures how far off the produced scores from step (2) are from the ground truth; and,\n",
    "4.   Define evaluation **metrics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import pandas as pd\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRAIN_DATA_PATH=\"../data/interim/train_sample_100.libsvm\"\n",
    "_TEST_DATA_PATH=\"../data/interim/val_sample_100.libsvm\"\n",
    "\n",
    "_LOSS=\"pairwise_logistic_loss\"\n",
    "# _LOSS=\"sigmoid_cross_entropy_loss\"\n",
    "\n",
    "_LIST_SIZE=10\n",
    "\n",
    "_NUM_FEATURES=4\n",
    "\n",
    "_BATCH_SIZE=1000\n",
    "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]\n",
    "\n",
    "#_OUT_DIR = \"../models/tfranking/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to read in data and form tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out train and test dataset is in the lib svm format which is normally used for Support Vector Machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 qid:10 1:4 2:47429 3:4604 4:14100\n",
      "\n",
      "100 qid:10 1:11 2:47796 3:7234 4:3000\n",
      "\n",
      "100 qid:10 1:8 2:49758 3:6878 4:5500\n",
      "\n",
      "100 qid:10 1:3 2:47429 3:4604\n",
      "\n",
      "100 qid:10 1:2 2:49067 3:6345 4:3100\n",
      "\n",
      "0 qid:10 1:1 2:48995 3:7396 4:3200\n",
      "\n",
      "100 qid:21 1:4 2:7667 3:1229 4:2100\n",
      "\n",
      "100 qid:21 1:3 2:7667 3:929\n",
      "\n",
      "0 qid:21 1:2 2:6157 3:1289 4:300\n",
      "\n",
      "100 qid:25 1:4 2:3898 3:669 4:1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fo = open(_TRAIN_DATA_PATH)\n",
    "i=0\n",
    "for f in fo:\n",
    "    if i != 10:\n",
    "        print(f)\n",
    "    else:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example they first number shows the target\n",
    "\n",
    "1 = Important \n",
    "\n",
    "0 = Not important\n",
    "\n",
    "qid: Describes which lines belong together\n",
    "\n",
    "E.g. query 10 had 6 suggestions for plans and just one is important. Here we would take the suggested transport mode from this plan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pipeline\n",
    "\n",
    "The first step to construct an input pipeline that reads your dataset and produces a `tensorflow.data.Dataset` object. In this example, we will invoke a LibSVM parser that is included in the `tensorflow_ranking.data` module to generate a `Dataset` from a given file.\n",
    "\n",
    "We parameterize this function by a `path` argument so that the function can be used to read both training and test data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Read tf DataSet\n",
    " \n",
    " Dic for feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(path):\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(path, _NUM_FEATURES, _LIST_SIZE),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
    "            for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.TensorShape([_LIST_SIZE])\n",
    "      )\n",
    "    )\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(1000).repeat().batch(_BATCH_SIZE)\n",
    "    return train_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({1: (10, 1), 2: (10, 1), 3: (10, 1), 4: (10, 1)}, (10,)), types: ({1: tf.float32, 2: tf.float32, 3: tf.float32, 4: tf.float32}, tf.float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "      tfr.data.libsvm_generator(_TRAIN_DATA_PATH, _NUM_FEATURES, _LIST_SIZE),\n",
    "      output_types=(\n",
    "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.float32\n",
    "      ),\n",
    "      output_shapes=(\n",
    "          {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
    "            for k in range(1,_NUM_FEATURES+1)},\n",
    "          tf.TensorShape([_LIST_SIZE])\n",
    "      )\n",
    "    )\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=train_dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': <tf.Tensor: id=83, shape=(10, 1), dtype=float32, numpy=\n",
       " array([[ 4.],\n",
       "        [ 8.],\n",
       "        [ 3.],\n",
       "        [11.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]], dtype=float32)>,\n",
       " '2': <tf.Tensor: id=84, shape=(10, 1), dtype=float32, numpy=\n",
       " array([[47429.],\n",
       "        [49758.],\n",
       "        [47429.],\n",
       "        [47796.],\n",
       "        [48995.],\n",
       "        [49067.],\n",
       "        [    0.],\n",
       "        [    0.],\n",
       "        [    0.],\n",
       "        [    0.]], dtype=float32)>,\n",
       " '3': <tf.Tensor: id=85, shape=(10, 1), dtype=float32, numpy=\n",
       " array([[4604.],\n",
       "        [6878.],\n",
       "        [4604.],\n",
       "        [7234.],\n",
       "        [7396.],\n",
       "        [6345.],\n",
       "        [   0.],\n",
       "        [   0.],\n",
       "        [   0.],\n",
       "        [   0.]], dtype=float32)>,\n",
       " '4': <tf.Tensor: id=86, shape=(10, 1), dtype=float32, numpy=\n",
       " array([[14100.],\n",
       "        [ 5500.],\n",
       "        [    0.],\n",
       "        [ 3000.],\n",
       "        [ 3200.],\n",
       "        [ 3100.],\n",
       "        [    0.],\n",
       "        [    0.],\n",
       "        [    0.],\n",
       "        [    0.]], dtype=float32)>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=87, shape=(10,), dtype=float32, numpy=\n",
       "array([100., 100., 100., 100.,   0., 100.,  -1.,  -1.,  -1.,  -1.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we turn to the scoring function which is arguably at the heart of a TF Ranking model. The idea is to compute a relevance score for a (set of) query-document pair(s). The TF-Ranking model will use training data to learn this function.\n",
    "\n",
    "Here we formulate a scoring function using a feed forward network. The function takes the features of a single example (i.e., query-document pair) and produces a relevance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_feature_columns():\n",
    "    \"\"\"Returns the example feature columns.\"\"\"\n",
    "    feature_names = [\n",
    "      \"%d\" % (i + 1) for i in range(0, _NUM_FEATURES)\n",
    "    ]\n",
    "    return {\n",
    "      name: tf.feature_column.numeric_column(\n",
    "          name, shape=(1,), default_value=0.0) for name in feature_names\n",
    "    }\n",
    "\n",
    "def make_score_fn():\n",
    "        \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\n",
    "        \n",
    "        def _score_fn(context_features, group_features, mode, params, config):\n",
    "            \"\"\"Defines the network to score a documents.\"\"\"\n",
    "            del params\n",
    "            del config\n",
    "            # Define input layer.\n",
    "            example_input = [\n",
    "                tf.layers.flatten(group_features[name])\n",
    "                for name in sorted(example_feature_columns())\n",
    "            ]\n",
    "            input_layer = tf.concat(example_input, 1)\n",
    "\n",
    "            cur_layer = input_layer\n",
    "            for i, layer_width in enumerate(int(d) for d in _HIDDEN_LAYER_DIMS):\n",
    "                cur_layer = tf.layers.dense(\n",
    "                  cur_layer,\n",
    "                  units=layer_width,\n",
    "                  activation=\"tanh\")\n",
    "\n",
    "            logits = tf.layers.dense(cur_layer, units=1)\n",
    "            return logits\n",
    "        return _score_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric_fns():\n",
    "    \"\"\"Returns a dict from name to metric functions.\n",
    "\n",
    "    This can be customized as follows. Care must be taken when handling padded\n",
    "    lists.\n",
    "\n",
    "    def _auc(labels, predictions, features):\n",
    "    is_label_valid = tf_reshape(tf.greater_equal(labels, 0.), [-1, 1])\n",
    "    clean_labels = tf.boolean_mask(tf.reshape(labels, [-1, 1], is_label_valid)\n",
    "    clean_pred = tf.boolean_maks(tf.reshape(predictions, [-1, 1], is_label_valid)\n",
    "    return tf.metrics.auc(clean_labels, tf.sigmoid(clean_pred), ...)\n",
    "    metric_fns[\"auc\"] = _auc\n",
    "\n",
    "    Returns:\n",
    "    A dict mapping from metric name to a metric function with above signature.\n",
    "    \"\"\"\n",
    "    metric_fns = {}\n",
    "    metric_fns.update({\n",
    "      \"metric/ndcg@%d\" % topn: tfr.metrics.make_ranking_metric_fn(\n",
    "          tfr.metrics.RankingMetricKey.NDCG, topn=topn)\n",
    "      for topn in [1, 3, 5, 10]\n",
    "    })\n",
    "\n",
    "    return metric_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(hparams):\n",
    "    \"\"\"Create a ranking estimator.\n",
    "\n",
    "    Args:\n",
    "    hparams: (tf.contrib.training.HParams) a hyperparameters object.\n",
    "\n",
    "    Returns:\n",
    "    tf.learn `Estimator`.\n",
    "    \"\"\"\n",
    "    def _train_op_fn(loss):\n",
    "        \"\"\"Defines train op used in ranking head.\"\"\"\n",
    "        return tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            learning_rate=hparams.learning_rate,\n",
    "            optimizer=\"Adagrad\")\n",
    "\n",
    "    ranking_head = tfr.head.create_ranking_head(\n",
    "      loss_fn=tfr.losses.make_loss_fn(_LOSS),\n",
    "      eval_metric_fns=eval_metric_fns(),\n",
    "      train_op_fn=_train_op_fn)\n",
    "\n",
    "    return tf.estimator.Estimator(\n",
    "      model_fn=tfr.model.make_groupwise_ranking_fn(\n",
    "          group_score_fn=make_score_fn(),\n",
    "          group_size=1,\n",
    "          transform_fn=None,\n",
    "          ranking_head=ranking_head),\n",
    "        params=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp8it4iksd\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp8it4iksd', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe19c2de390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(learning_rate=0.05)\n",
    "ranker = get_estimator(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n",
      "WARNING:tensorflow:From /home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/model.py:102: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-28-b17da5dc3134>:21: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-28-b17da5dc3134>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/sandro/.local/lib/python3.7/site-packages/tensorflow_ranking/python/model.py:268: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandro/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp8it4iksd/model.ckpt.\n",
      "INFO:tensorflow:loss = 74.1089, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /tmp/tmp8it4iksd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 61.284252.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7fe19c2d8ef0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.train(input_fn=lambda: input_fn(_TRAIN_DATA_PATH), steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 1.285e+03, 1.088e+03, 2.000e+02],\n",
       "       [3.000e+00, 9.160e+02, 1.880e+02, 0.000e+00],\n",
       "       [5.000e+00, 9.230e+02, 8.590e+02, 0.000e+00],\n",
       "       ...,\n",
       "       [1.000e+00, 6.102e+03, 2.547e+03, 2.000e+02],\n",
       "       [1.000e+00, 4.217e+03, 1.926e+03, 2.000e+02],\n",
       "       [3.000e+00, 7.594e+03, 1.181e+03, 0.000e+00]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ranker.predict(input_fn=lambda: (\"../data/interim/val_sample_100_wo_target.libsvm.txt\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-51b2a118c192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    609\u001b[0m             input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[1;32m    610\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 611\u001b[0;31m             features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_ranking/python/model.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, params, config)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mtf_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Use groupwise dnn v2.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_groupwise_dnn_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     return ranking_head.create_estimator_spec(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_ranking/python/model.py\u001b[0m in \u001b[0;36m_groupwise_dnn_v2\u001b[0;34m(features, labels, mode, params, config)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m       context_features, per_example_features = _call_transform_fn(\n\u001b[0;32m--> 188\u001b[0;31m           features, mode)\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_score_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_ranking/python/model.py\u001b[0m in \u001b[0;36m_call_transform_fn\u001b[0;34m(features, mode)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mtransform_fn_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'mode'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransform_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_ranking/python/feature.py\u001b[0m in \u001b[0;36m_transform_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     51\u001b[0m     context_features = {\n\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_feature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     }\n",
      "\u001b[0;32m~/anaconda3/envs/tf_ranking/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36miteritems\u001b[0;34m(d, **kw)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miterlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltr_to_submission(df, features, ranker, path):\n",
    "    features = features + ['sid']\n",
    "    \n",
    "    preds = ranker.predict(input_fn=lambda: input_fn(path))\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    # Not sure how to get all preds because it runs infinit\n",
    "    # So I take all till list size\n",
    "    preds_slice = itertools.islice(preds, len(df)) \n",
    "    count=0\n",
    "    a = np.zeros((len(df), _LIST_SIZE))\n",
    "\n",
    "    for i in preds_slice:\n",
    "        a[count]=i\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "    test_X = df[features]\n",
    "    \n",
    "    # Assign prediction vals to df\n",
    "    # Tried with a or a sum for all features\n",
    "    \n",
    "    # --- Sum of the first n feature values --- \n",
    "    # a = a[:,0:_NUM_FEATURES]\n",
    "    # a = a.sum(axis=1)\n",
    "    \n",
    "    # test_X = test_X.assign(yhat = a)\n",
    "    \n",
    "     # --- Just the feature prio where the transport mode is in --- \n",
    "    \n",
    "    test_X = test_X.assign(yhat = a[:,0])\n",
    "    \n",
    "    df_end = pd.DataFrame(columns=['yhat'], index=df.sid.unique())\n",
    "\n",
    "    df_end = test_X.sort_values(['sid', 'yhat'], ascending=False).groupby('sid').first()[[\n",
    "        'yhat', 'transport_mode'\n",
    "    ]]\n",
    "    # return df_end\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    score = f1_score(df.groupby(\"sid\").first()['click_mode'], df_end.transport_mode, average='weighted')\n",
    "    print('F1 Score is: {}'.format(score))\n",
    "    \n",
    "    return df_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_train = pd.read_pickle(\"../data/interim/train_row.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test = pd.read_pickle(\"../data/interim/val_row.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>click_mode</th>\n",
       "      <th>distance_plan</th>\n",
       "      <th>eta</th>\n",
       "      <th>price</th>\n",
       "      <th>transport_mode</th>\n",
       "      <th>plan_time</th>\n",
       "      <th>pid</th>\n",
       "      <th>req_time</th>\n",
       "      <th>o_long</th>\n",
       "      <th>...</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>wind</th>\n",
       "      <th>dy</th>\n",
       "      <th>dyq</th>\n",
       "      <th>q</th>\n",
       "      <th>qdy</th>\n",
       "      <th>xq</th>\n",
       "      <th>xydy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1258547</th>\n",
       "      <td>697184.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7594.0</td>\n",
       "      <td>1361.0</td>\n",
       "      <td>2200</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-11-15 05:16:06</td>\n",
       "      <td>132790.0</td>\n",
       "      <td>2018-11-15 05:16:06</td>\n",
       "      <td>116.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258549</th>\n",
       "      <td>697184.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3932.0</td>\n",
       "      <td>3605.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-11-15 05:16:06</td>\n",
       "      <td>132790.0</td>\n",
       "      <td>2018-11-15 05:16:06</td>\n",
       "      <td>116.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272907</th>\n",
       "      <td>673286.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9558.0</td>\n",
       "      <td>2875.0</td>\n",
       "      <td>400</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-15 05:16:52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-15 05:16:52</td>\n",
       "      <td>116.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272911</th>\n",
       "      <td>673286.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>3373.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-11-15 05:16:52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-15 05:16:52</td>\n",
       "      <td>116.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272909</th>\n",
       "      <td>673286.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7916.0</td>\n",
       "      <td>2390.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2018-11-15 05:16:52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-15 05:16:52</td>\n",
       "      <td>116.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              sid  click_mode  distance_plan     eta  price  transport_mode  \\\n",
       "1258547  697184.0         1.0         7594.0  1361.0   2200             4.0   \n",
       "1258549  697184.0         1.0         3932.0  3605.0      0             5.0   \n",
       "272907   673286.0         2.0         9558.0  2875.0    400             2.0   \n",
       "272911   673286.0         2.0         8900.0  3373.0    400             1.0   \n",
       "272909   673286.0         2.0         7916.0  2390.0      0             6.0   \n",
       "\n",
       "                   plan_time       pid            req_time  o_long  ...  \\\n",
       "1258547  2018-11-15 05:16:06  132790.0 2018-11-15 05:16:06  116.42  ...   \n",
       "1258549  2018-11-15 05:16:06  132790.0 2018-11-15 05:16:06  116.42  ...   \n",
       "272907   2018-11-15 05:16:52       0.0 2018-11-15 05:16:52  116.37  ...   \n",
       "272911   2018-11-15 05:16:52       0.0 2018-11-15 05:16:52  116.37  ...   \n",
       "272909   2018-11-15 05:16:52       0.0 2018-11-15 05:16:52  116.37  ...   \n",
       "\n",
       "         is_holiday  max_temp  min_temp  wind  dy  dyq  q  qdy  xq  xydy  \n",
       "1258547           0         8         1    34   0    0  0    0   0     1  \n",
       "1258549           0         8         1    34   0    0  0    0   0     1  \n",
       "272907            0         8         1    34   0    0  0    0   0     1  \n",
       "272911            0         8         1    34   0    0  0    0   0     1  \n",
       "272909            0         8         1    34   0    0  0    0   0     1  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_pickle(\"../data/interim/features_row_sample.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpw4umrvzi/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "F1 Score is: 0.317192514928933\n"
     ]
    }
   ],
   "source": [
    "df_preds = ltr_to_submission(df_train_test, features, ranker, _TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end = test_X[msk][['sid', 'transport_mode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143113     0.509255\n",
       "143110     0.509255\n",
       "143112     0.509255\n",
       "143109     0.509255\n",
       "459059     0.509255\n",
       "459057     0.509255\n",
       "459061     0.509255\n",
       "459062     0.509255\n",
       "459058     0.509255\n",
       "459060     0.509255\n",
       "909545     0.509255\n",
       "909546     0.509255\n",
       "909548     0.509255\n",
       "909547     0.509255\n",
       "241776    -1.318469\n",
       "241774    -1.318469\n",
       "241775    -1.318469\n",
       "241773    -1.318469\n",
       "241777    -1.318469\n",
       "1270163    0.509255\n",
       "1270164    0.509255\n",
       "1270165    0.509255\n",
       "1270168    0.509255\n",
       "1270167    0.509255\n",
       "1270166    0.509255\n",
       "316955     0.509255\n",
       "316959     0.509255\n",
       "316957     0.509255\n",
       "316958     0.509255\n",
       "316956     0.509255\n",
       "             ...   \n",
       "1525598    0.000000\n",
       "1525603    0.000000\n",
       "1525602    0.000000\n",
       "1525599    0.000000\n",
       "1525600    0.000000\n",
       "45680      0.000000\n",
       "45682      0.000000\n",
       "45681      0.000000\n",
       "45678      0.000000\n",
       "45679      0.000000\n",
       "45683      0.000000\n",
       "1449538    0.000000\n",
       "1449537    0.000000\n",
       "1449539    0.000000\n",
       "1449535    0.000000\n",
       "1449536    0.000000\n",
       "1326911    0.000000\n",
       "1326910    0.000000\n",
       "1326912    0.000000\n",
       "1326909    0.000000\n",
       "1326908    0.000000\n",
       "17801      0.000000\n",
       "17802      0.000000\n",
       "17800      0.000000\n",
       "17798      0.000000\n",
       "17799      0.000000\n",
       "861591     0.000000\n",
       "861590     0.000000\n",
       "861592     0.000000\n",
       "861593     0.000000\n",
       "Name: yhat, Length: 601059, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.groupby('sid', sort=False)['yhat'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['new']=test_X.groupby('sid')['yhat'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129542, 6)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.sort_values(['sid', 'yhat'], ascending=False).groupby('sid').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600983, 2)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_end.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Use groupwise dnn v2.\n",
      "MAKE SCORE FUNCTION:\n",
      "[<tf.Tensor 'groupwise_dnn_v2/group_score/flatten/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_1/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_2/Reshape:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'groupwise_dnn_v2/group_score/flatten_3/Reshape:0' shape=(?, 1) dtype=float32>]\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp_09c8s1t/model.ckpt-1100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "preds = ranker.predict(input_fn=lambda: input_fn(_TRAIN_DATA_PATH))\n",
    "import itertools\n",
    "import numpy as np\n",
    "# Not sure how to get all preds because it runs infinit\n",
    "# So I take all till list size\n",
    "preds_slice = itertools.islice(preds, len(df_train_test)) \n",
    "count=0\n",
    "a = np.zeros((len(df_train_test), _LIST_SIZE))\n",
    "\n",
    "for i in preds_slice:\n",
    "    a[count]=i\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=a[:,0:3]\n",
    "a = a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = df_train_test[[\n",
    "       'sid',\n",
    "        'transport_mode',\n",
    "        'distance_plan',\n",
    "        'eta', \n",
    "        'price'\n",
    "    ]]\n",
    "    \n",
    "# Assign prediction vals to df\n",
    "test_X = test_X.assign(yhat = a)\n",
    "\n",
    "df_end = pd.DataFrame(columns=['yhat'], index=df_train_test.sid.unique())\n",
    "\n",
    "df_end = test_X.sort_values(['sid', 'yhat'], ascending=False).groupby('sid').first()[[\n",
    "    'yhat', 'transport_mode'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3075881119766427"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(df_train_train.groupby(\"sid\").first()['click_mode'], df_end.transport_mode, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try datasets from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_train = pd.read_pickle(\"../data/interim/train_row.pickle\")\n",
    "features = pd.read_pickle(\"../data/interim/features_row_sample.pickle\")\n",
    "train = df_train_train[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transport_mode</th>\n",
       "      <th>distance_plan</th>\n",
       "      <th>eta</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1027273</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027271</th>\n",
       "      <td>3.0</td>\n",
       "      <td>916.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027270</th>\n",
       "      <td>5.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>859.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027272</th>\n",
       "      <td>6.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814953</th>\n",
       "      <td>2.0</td>\n",
       "      <td>35361.0</td>\n",
       "      <td>5259.0</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         transport_mode  distance_plan     eta  price\n",
       "1027273             1.0         1285.0  1088.0    200\n",
       "1027271             3.0          916.0   188.0      0\n",
       "1027270             5.0          923.0   859.0      0\n",
       "1027272             6.0          923.0   279.0      0\n",
       "1814953             2.0        35361.0  5259.0    700"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_train[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train_train.click_mode.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = tf.data.Dataset.from_tensor_slices((train, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(path):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train, labels))\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(1000).repeat().batch(_BATCH_SIZE)\n",
    "    return train_dataset.make_one_shot_iterator().get_next()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
